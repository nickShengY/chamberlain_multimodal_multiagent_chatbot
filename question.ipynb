{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "# from openai import OpenAI\n",
    "OpenAI_API_KEY = \"\"\n",
    "openai.api_key = OpenAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI_API_KEY = \"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\"\n",
    "# openai.api_key = OpenAI_API_KEY\n",
    "model_id = 'gpt-3.5-turbo'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the Number  0 Prompts\n",
      "Processing the Number  10 Prompts\n",
      "Processing the Number  20 Prompts\n",
      "Processing the Number  30 Prompts\n",
      "Processing the Number  40 Prompts\n",
      "Processing the Number  50 Prompts\n",
      "Processing the Number  60 Prompts\n",
      "Processing the Number  70 Prompts\n",
      "Processing the Number  80 Prompts\n",
      "Processing the Number  90 Prompts\n",
      "Processing the Number  100 Prompts\n",
      "Processing the Number  110 Prompts\n",
      "Processing the Number  120 Prompts\n",
      "Processing the Number  130 Prompts\n",
      "Processing the Number  140 Prompts\n",
      "Processing the Number  150 Prompts\n",
      "Processing the Number  160 Prompts\n",
      "Processing the Number  170 Prompts\n",
      "Processing the Number  180 Prompts\n",
      "Processing the Number  190 Prompts\n",
      "Processing the Number  200 Prompts\n",
      "Processing the Number  210 Prompts\n",
      "Processing the Number  220 Prompts\n",
      "Processing the Number  230 Prompts\n",
      "Processing the Number  240 Prompts\n",
      "Processing the Number  250 Prompts\n",
      "Processing the Number  260 Prompts\n",
      "Processing the Number  270 Prompts\n",
      "Processing the Number  280 Prompts\n",
      "Processing the Number  290 Prompts\n",
      "Processing the Number  300 Prompts\n",
      "Processing the Number  310 Prompts\n",
      "Processing the Number  320 Prompts\n",
      "Processing the Number  330 Prompts\n",
      "Processing the Number  340 Prompts\n",
      "Processing the Number  350 Prompts\n",
      "Processing the Number  360 Prompts\n",
      "Processing the Number  370 Prompts\n",
      "Processing the Number  380 Prompts\n",
      "Processing the Number  390 Prompts\n",
      "Processing the Number  400 Prompts\n",
      "Processing the Number  410 Prompts\n",
      "Processing the Number  420 Prompts\n",
      "Processing the Number  430 Prompts\n",
      "Processing the Number  440 Prompts\n",
      "Processing the Number  450 Prompts\n",
      "Processing the Number  460 Prompts\n",
      "Processing the Number  470 Prompts\n",
      "Processing the Number  480 Prompts\n",
      "Processing the Number  490 Prompts\n",
      "Processing the Number  500 Prompts\n",
      "Processing the Number  510 Prompts\n",
      "Processing the Number  520 Prompts\n",
      "Processing the Number  530 Prompts\n",
      "Processing the Number  540 Prompts\n",
      "Processing the Number  550 Prompts\n",
      "Processing the Number  560 Prompts\n",
      "Processing the Number  570 Prompts\n",
      "Processing the Number  580 Prompts\n",
      "Processing the Number  590 Prompts\n",
      "Processing the Number  600 Prompts\n",
      "Processing the Number  610 Prompts\n",
      "Processing the Number  620 Prompts\n",
      "Processing the Number  630 Prompts\n",
      "Processing the Number  640 Prompts\n",
      "Processing the Number  650 Prompts\n",
      "Processing the Number  660 Prompts\n",
      "Processing the Number  670 Prompts\n",
      "Processing the Number  680 Prompts\n",
      "Processing the Number  690 Prompts\n",
      "                                            Question Label\n",
      "0   1. What are some delicious meal ideas for today?   Eat\n",
      "1  2. Can you suggest a tasty and healthy dish fo...   Eat\n",
      "2  3. I need some inspiration for what to cook/ea...   Eat\n",
      "3  4. What types of food do you recommend I consu...   Eat\n",
      "4  5. I'm looking for suggestions on what to eat ...   Eat\n",
      "5  6. Can you give me some options for what I sho...   Eat\n",
      "6  7. What are some good food options for today's...   Eat\n",
      "7  8. Any suggestions for a satisfying and nutrit...   Eat\n",
      "8  9. I'm looking for ideas on what to prepare/ea...   Eat\n",
      "9  10. Can you offer some guidance on what I shou...   Eat\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the prompts\n",
    "prompts = {\n",
    "    \"Eat\": \"You need to help me come up with 10 potential similar questions, that very similar to the question: What should i eat today?\",\n",
    "    \"Dress\": \"You need to help me come up with 10 potential similar questions, that very similar to the question: What should i wear today? or do i look good in these?\",\n",
    "    \"Bill\": \"You need to help me come up with 10 potential daily questions the users might be interested in regarding if there is a bill due recently\",\n",
    "    \"Finance\": \"You need to help me come up with 10 potential daily questions the users might be interested in asking personal financial suggestions based on his current portfolio or financial documents\",\n",
    "    \"Planner\": \"You need to help me come up with 10 potential daily questions the users might be interested in regarding how to plan a schedule\",\n",
    "    \"Grocery\": \"You need to help me come up with 10 potential daily questions or prompt after the users have just finished shopping grocery\",\n",
    "    \"Lanudry\": \"You need to help me come up with 10 potential daily prompt when the users want to do lanudry\",\n",
    "    \"Fun\": \"You need to help me come up with 10 potential daily prompt when the users want to go out for an event at certain time (concert, movie theater, music show, sightseeing, bar, club, sport games))\",\n",
    "    \"IoT\": \"You need to help me come up with 10 potential daily prompt when the users want to use tv, computer, home theater, netflix, etc.\",\n",
    "    \"Shopping\": \"You need to help me come up with 10 potential daily prompt after the users have just finished shopping or they want to shop online\",\n",
    "    \"Flight\": \"You need to help me come up with 10 potential similar questions, that very similar to the request: I want to go travel\",\n",
    "    \"Coding\": \"You need to help me come up with 10 potential similar questions, or prompts that very similar to the request: I want you to develop an app for me\",\n",
    "    \"Task\": \"You need to help me come up with 10 potential similar questions, or prompts that very similar to the request: I have a hard task today, you need to help me\",\n",
    "    \"Other\": \"You need to help me come up with 10 random daily questions/prompts or factual questions that are not related to Eat, Dress, Bill payment, finance suggestions, portfolio analysis, personal planer, grocery, laundry, going out for an event, watching tv or playing computer games, shopping, booking a flight, anything related to developing an app or game, planning schedule for a personal task or finishing a task\",\n",
    "}\n",
    "\n",
    "# Initialize the dataset\n",
    "questions_dataset = pd.DataFrame(columns=['Question', 'Label'])\n",
    "\n",
    "# Function to make an API call and handle errors\n",
    "def make_api_call(prompt_label):\n",
    "    try:\n",
    "        # API call\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model_id,\n",
    "            messages=[{\"role\": \"system\", \"content\": prompts[prompt_label]}]\n",
    "        )\n",
    "\n",
    "        # Process response\n",
    "        response_text = response.choices[-1].message.content.strip()\n",
    "        # print(response_text)\n",
    "        # Assuming each response contains multiple questions, split them\n",
    "        questions = response_text.split('\\n')\n",
    "\n",
    "        # Add questions to the dataset\n",
    "        for question in questions:\n",
    "            if question:  # Ensure the question is not empty\n",
    "                questions_dataset.loc[len(questions_dataset)] = [question, prompt_label]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}. Retrying...\")\n",
    "        make_api_call(prompt_label)  # Retry on error\n",
    "\n",
    "# Call the function for each prompt\n",
    "for n in range(700):\n",
    "    if n % 10 == 0:\n",
    "        print(\"Processing the Number \", n, \"Prompts\")\n",
    "    for label in prompts:\n",
    "        make_api_call(label)\n",
    "\n",
    "# Check some of the collected questions\n",
    "print(questions_dataset.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_dataset.to_csv(\"questions_dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_question(question):\n",
    "    # Remove leading numbers and punctuation (e.g., '1. ', '2. ')\n",
    "    question = re.sub(r'^\\d+\\.\\s+', '', question)\n",
    "\n",
    "    # Remove extra quotes if needed\n",
    "    question = question.replace('\"\"', '\"')\n",
    "    question = question.replace('\"', '')\n",
    "    question = question.replace('\"', '')\n",
    "\n",
    "    # Additional cleaning and standardization can be added here\n",
    "\n",
    "    return question\n",
    "\n",
    "# Apply the function to the 'Question' column\n",
    "questions_dataset['Question'] = questions_dataset['Question'].apply(clean_question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_dataset.to_csv(\"questions_dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Eat', 'Dress', 'Bill', 'Finance', 'Planner', 'Grocery', 'Lanudry',\n",
       "       'Fun', 'IoT', 'Shopping', 'Flight', 'Coding', 'Task', 'Other'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_dataset[\"Label\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Step 2: Prepare your dataset\n",
    "# Load your dataset (replace 'your_dataset.csv' with the actual file path)\n",
    "df = pd.read_csv('questions_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map labels to integers\n",
    "label_map = {label: idx for idx, label in enumerate(df['Label'].unique())}\n",
    "df['Label'] = df['Label'].map(label_map)\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "train_texts, temp_texts, train_labels, temp_labels = train_test_split(df['Question'], df['Label'], test_size=0.3)\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(temp_texts, temp_labels, test_size=0.5)\n",
    "\n",
    "\n",
    "# Step 3: Tokenize the data\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "# Reindex the datasets\n",
    "train_texts = train_texts.reset_index(drop=True)\n",
    "test_texts = test_texts.reset_index(drop=True)\n",
    "val_texts = val_texts.reset_index(drop=True)\n",
    "\n",
    "train_labels = train_labels.reset_index(drop=True)\n",
    "test_labels = test_labels.reset_index(drop=True)\n",
    "val_labels = val_labels.reset_index(drop=True)\n",
    "\n",
    "\n",
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)\n",
    "val_encodings = tokenizer(list(val_texts), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(list(test_texts), truncation=True, padding=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Create a Dataset class\n",
    "class QuestionsDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = QuestionsDataset(train_encodings, train_labels)\n",
    "val_dataset = QuestionsDataset(val_encodings, val_labels)\n",
    "test_dataset = QuestionsDataset(test_encodings, test_labels)\n",
    "\n",
    "# Step 5: Initialize the RoBERTa model\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=len(label_map))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\DeepL\\.venv\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Validation Loss: 0.0023660638835281134\n",
      "Epoch 1: Validation Loss: 0.0007686428143642843\n",
      "Epoch 2: Validation Loss: 0.008991550654172897\n",
      "Epoch 3: Validation Loss: 0.000932412629481405\n",
      "Epoch 4: Validation Loss: 0.0004400273319333792\n",
      "Epoch 5: Validation Loss: 0.0018445459427312016\n",
      "Epoch 6: Validation Loss: 0.0009188526310026646\n",
      "Epoch 7: Validation Loss: 0.00029747249209322035\n",
      "Epoch 8: Validation Loss: 0.0009828642942011356\n",
      "Epoch 9: Validation Loss: 0.0012218529591336846\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Train the model\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Early stopping parameters\n",
    "early_stopping_patience = 3\n",
    "early_stopping_counter = 0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "for epoch in range(10):  # number of epochs\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        \n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            # ... validation step ...\n",
    "\n",
    "            val_loss += outputs.loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f'Epoch {epoch}: Validation Loss: {val_loss}')\n",
    "\n",
    "    # Check for early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stopping_counter = 0\n",
    "        torch.save(model.state_dict(), 'question_model.pt')  # Save the best model\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Evaluate the model\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('question_model.pt'))\n",
    "\n",
    "\n",
    "model.eval()\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "test_preds, test_labels_list = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        logits = outputs.logits\n",
    "        test_preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "        test_labels_list.extend(labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9957371946681102\n",
      "Precision: 0.9957868935988617, Recall: 0.9957718437689103\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "accuracy = accuracy_score(test_labels_list, test_preds)\n",
    "precision, recall, _, _ = precision_recall_fscore_support(test_labels_list, test_preds, average='macro')\n",
    "print(f'Test Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}, Recall: {recall}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=14, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=len(label_map))\n",
    "model.load_state_dict(torch.load('question_model.pt'))\n",
    "\n",
    "# If you are using a GPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_question(question, model, tokenizer, label_map):\n",
    "    # Tokenize the question\n",
    "    inputs = tokenizer(question, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "\n",
    "    # Move inputs to the same device as the model\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Model inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get the predicted class\n",
    "    predicted_class_idx = outputs.logits.argmax(dim=1).item()\n",
    "    print(outputs.logits)\n",
    "    # Map the predicted index to the corresponding label\n",
    "    predicted_class = [label for label, idx in label_map.items() if idx == predicted_class_idx][0]\n",
    "\n",
    "    return predicted_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.6947,  0.7644,  1.5115,  0.1594,  2.1943, -1.2402, -1.9550, -1.0146,\n",
      "         -0.6065, -0.9485, -0.9338, -0.5219,  2.3150, -1.1989]],\n",
      "       device='cuda:0')\n",
      "Predicted Label: Eat\n"
     ]
    }
   ],
   "source": [
    "question = \"I am Nick\"\n",
    "predicted_label = predict_question(question, model, tokenizer, label_map)\n",
    "print(f\"Predicted Label: {predicted_label}\")\n",
    "# For date, tired, eat  -- it can be eating, so ask them if they want to have something to eat then\n",
    "# For weather, they might want to go out for fun\n",
    "# sad is other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the Number  0 Prompts\n",
      "Processing the Number  10 Prompts\n",
      "Processing the Number  20 Prompts\n",
      "Processing the Number  30 Prompts\n",
      "Processing the Number  40 Prompts\n",
      "Processing the Number  50 Prompts\n",
      "Processing the Number  60 Prompts\n",
      "Processing the Number  70 Prompts\n",
      "Processing the Number  80 Prompts\n",
      "Processing the Number  90 Prompts\n",
      "Processing the Number  100 Prompts\n",
      "Processing the Number  110 Prompts\n",
      "Processing the Number  120 Prompts\n",
      "Processing the Number  130 Prompts\n",
      "Processing the Number  140 Prompts\n",
      "Processing the Number  150 Prompts\n",
      "Processing the Number  160 Prompts\n",
      "Processing the Number  170 Prompts\n",
      "Processing the Number  180 Prompts\n",
      "Processing the Number  190 Prompts\n",
      "Processing the Number  200 Prompts\n",
      "Processing the Number  210 Prompts\n",
      "Processing the Number  220 Prompts\n",
      "Processing the Number  230 Prompts\n",
      "Processing the Number  240 Prompts\n",
      "Processing the Number  250 Prompts\n",
      "Processing the Number  260 Prompts\n",
      "Processing the Number  270 Prompts\n",
      "Processing the Number  280 Prompts\n",
      "Processing the Number  290 Prompts\n",
      "Processing the Number  300 Prompts\n",
      "Processing the Number  310 Prompts\n",
      "Processing the Number  320 Prompts\n",
      "Processing the Number  330 Prompts\n",
      "Processing the Number  340 Prompts\n",
      "Processing the Number  350 Prompts\n",
      "Processing the Number  360 Prompts\n",
      "Processing the Number  370 Prompts\n",
      "Processing the Number  380 Prompts\n",
      "Processing the Number  390 Prompts\n",
      "Processing the Number  400 Prompts\n",
      "Processing the Number  410 Prompts\n",
      "Processing the Number  420 Prompts\n",
      "Processing the Number  430 Prompts\n",
      "Processing the Number  440 Prompts\n",
      "Processing the Number  450 Prompts\n",
      "Processing the Number  460 Prompts\n",
      "Processing the Number  470 Prompts\n",
      "Processing the Number  480 Prompts\n",
      "Processing the Number  490 Prompts\n",
      "Processing the Number  500 Prompts\n",
      "Processing the Number  510 Prompts\n",
      "Processing the Number  520 Prompts\n",
      "Processing the Number  530 Prompts\n",
      "Processing the Number  540 Prompts\n",
      "Processing the Number  550 Prompts\n",
      "Processing the Number  560 Prompts\n",
      "Processing the Number  570 Prompts\n",
      "Processing the Number  580 Prompts\n",
      "Processing the Number  590 Prompts\n",
      "Processing the Number  600 Prompts\n",
      "Processing the Number  610 Prompts\n",
      "Processing the Number  620 Prompts\n",
      "Processing the Number  630 Prompts\n",
      "Processing the Number  640 Prompts\n",
      "Processing the Number  650 Prompts\n",
      "Processing the Number  660 Prompts\n",
      "Processing the Number  670 Prompts\n",
      "Processing the Number  680 Prompts\n",
      "Processing the Number  690 Prompts\n",
      "                                               judge Label\n",
      "0              1. No, I'm sorry, I don't understand.    No\n",
      "1                     2. I'm not sure what you mean.    No\n",
      "2  3. I don't think I can provide an answer to that.    No\n",
      "3                    4. No, that doesn't make sense.    No\n",
      "4    5. I'm afraid I can't comprehend your question.    No\n",
      "5    6. I'm not sure I can assist with that inquiry.    No\n",
      "6  7. No, I don't have the information you're see...    No\n",
      "7  8. I'm sorry, but I can't give you a definitiv...    No\n",
      "8           9. No, I'm not familiar with that topic.    No\n",
      "9  10. I don't believe I have enough knowledge to...    No\n"
     ]
    }
   ],
   "source": [
    "# Define the prompts\n",
    "prompts = {\n",
    "    \"No\": \"Imagine if you are trying to understand the answer of a chatbot, you need to generate 10 potential simple negation and short type of response or they say they don't understand or they are not sure. Just give me the responses, no other explanation needed\",\n",
    "    \"Yes\": \"Imagine you are trying to understand the answer of a chatbot, you need to generate 10 potential simple and short positive type of response.\"\n",
    "}\n",
    "\n",
    "# Initialize the dataset\n",
    "label_dataset = pd.DataFrame(columns=['judge', 'Label'])\n",
    "\n",
    "# Function to make an API call and handle errors\n",
    "def make_api_call(prompt_label):\n",
    "    try:\n",
    "        # API call\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model_id,\n",
    "            messages=[{\"role\": \"system\", \"content\": prompts[prompt_label]}]\n",
    "        )\n",
    "\n",
    "        # Process response\n",
    "        response_text = response.choices[-1].message.content.strip()\n",
    "        # print(response_text)\n",
    "        # Assuming each response contains multiple questions, split them\n",
    "        judges = response_text.split('\\n')\n",
    "\n",
    "        # Add questions to the dataset\n",
    "        for judge in judges:\n",
    "            if judge:  # Ensure the question is not empty\n",
    "                label_dataset.loc[len(label_dataset)] = [judge, prompt_label]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}. Retrying...\")\n",
    "        make_api_call(prompt_label)  # Retry on error\n",
    "\n",
    "# Call the function for each prompt\n",
    "for n in range(700):\n",
    "    if n % 10 == 0:\n",
    "        print(\"Processing the Number \", n, \"Prompts\")\n",
    "    for label in prompts:\n",
    "        make_api_call(label)\n",
    "\n",
    "# Check some of the collected questions\n",
    "print(label_dataset.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dataset.to_csv(\"label_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dataset['judge'] = label_dataset['judge'].apply(clean_question)\n",
    "label_dataset.to_csv(\"label_dataset.csv\")\n",
    "df = pd.read_csv('label_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map labels to integers\n",
    "label_map = {label: idx for idx, label in enumerate(df['Label'].unique())}\n",
    "df['Label'] = df['Label'].map(label_map)\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "train_texts, temp_texts, train_labels, temp_labels = train_test_split(df['judge'], df['Label'], test_size=0.3)\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(temp_texts, temp_labels, test_size=0.5)\n",
    "\n",
    "\n",
    "# Step 3: Tokenize the data\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "# Reindex the datasets\n",
    "train_texts = train_texts.reset_index(drop=True)\n",
    "test_texts = test_texts.reset_index(drop=True)\n",
    "val_texts = val_texts.reset_index(drop=True)\n",
    "\n",
    "train_labels = train_labels.reset_index(drop=True)\n",
    "test_labels = test_labels.reset_index(drop=True)\n",
    "val_labels = val_labels.reset_index(drop=True)\n",
    "\n",
    "\n",
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)\n",
    "val_encodings = tokenizer(list(val_texts), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(list(test_texts), truncation=True, padding=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Create a Dataset class\n",
    "class QuestionsDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = QuestionsDataset(train_encodings, train_labels)\n",
    "val_dataset = QuestionsDataset(val_encodings, val_labels)\n",
    "test_dataset = QuestionsDataset(test_encodings, test_labels)\n",
    "\n",
    "# Step 5: Initialize the RoBERTa model\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=len(label_map))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\DeepL\\.venv\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Validation Loss: 0.001739870524033904\n",
      "Epoch 1: Validation Loss: 0.0001770163216860965\n",
      "Epoch 2: Validation Loss: 9.797151142265648e-05\n",
      "Epoch 3: Validation Loss: 7.709840429015458e-05\n",
      "Epoch 4: Validation Loss: 0.00011216704297112301\n",
      "Epoch 5: Validation Loss: 4.409285611473024e-05\n",
      "Epoch 6: Validation Loss: 2.5073277356568724e-05\n",
      "Epoch 7: Validation Loss: 1.1947315215365961e-05\n",
      "Epoch 8: Validation Loss: 1.213277573697269e-05\n",
      "Epoch 9: Validation Loss: 4.781604275194695e-06\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Train the model\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Early stopping parameters\n",
    "early_stopping_patience = 3\n",
    "early_stopping_counter = 0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "for epoch in range(10):  # number of epochs\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        \n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            # ... validation step ...\n",
    "\n",
    "            val_loss += outputs.loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f'Epoch {epoch}: Validation Loss: {val_loss}')\n",
    "\n",
    "    # Check for early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stopping_counter = 0\n",
    "        torch.save(model.state_dict(), 'label_model.pt')  # Save the best model\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Evaluate the model\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('label_model.pt'))\n",
    "\n",
    "\n",
    "model.eval()\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "test_preds, test_labels_list = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        logits = outputs.logits\n",
    "        test_preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "        test_labels_list.extend(labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9995240361732508\n",
      "Precision: 0.9995265151515151, Recall: 0.9995219885277247\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(test_labels_list, test_preds)\n",
    "precision, recall, _, _ = precision_recall_fscore_support(test_labels_list, test_preds, average='macro')\n",
    "print(f'Test Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}, Recall: {recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
    "model.load_state_dict(torch.load('label_model.pt'))\n",
    "\n",
    "# If you are using a GPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.6067,  3.7065]], device='cuda:0')\n",
      "Predicted Label: Yes\n"
     ]
    }
   ],
   "source": [
    "question = \"Yea\"\n",
    "predicted_label = predict_question(question, model, tokenizer, label_map)\n",
    "print(f\"Predicted Label: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
